{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0rm9shTqFJq",
        "outputId": "334c3671-4988-40fa-89c4-363da061e86b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/amcar/Desktop/climate_lens/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "path_pop = kagglehub.dataset_download(\"iamsouravbanerjee/world-population-dataset\")\n",
        "df_pop = pd.read_csv(f\"{path_pop}/world_population.csv\")\n",
        "\n",
        "# 2. Annual CO2 Emissions by Country\n",
        "path_co2 = kagglehub.dataset_download(\"ulrikthygepedersen/co2-emissions-by-country\")\n",
        "df_co2 = pd.read_csv(f\"{path_co2}/co2_emissions_kt_by_country.csv\")\n",
        "\n",
        "# 3. Monthly Climate Data by Station\n",
        "path_station = kagglehub.dataset_download(\"christopherlemke/monthly-climat-reports-from-stations-worldwide\")\n",
        "df_reports = pd.read_csv(f\"{path_station}/dwd-cdc_CLIMAT_reports_stations_ww.csv\")\n",
        "df_stations = pd.read_csv(f\"{path_station}/dwd-cdc_station_data_ww.csv\")\n",
        "\n",
        "path_aq1 = kagglehub.dataset_download(\"kanchana1990/world-air-quality-data-2024-updated\")\n",
        "df_aq1 = pd.read_csv(f\"{path_aq1}/world_air_quality.csv\", on_bad_lines='skip', sep=';')\n",
        "\n",
        "path_aq2 = kagglehub.dataset_download(\"dnkumars/air-quality-index\")\n",
        "df_aq2 = pd.read_csv(f\"{path_aq2}/cleaned_interpolated_dataset.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z55LLUdWjJg7"
      },
      "source": [
        "## Co2 DF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QDBijafk8rin"
      },
      "outputs": [],
      "source": [
        "co2_countries = df_co2['country_code'].unique()\n",
        "pop_countries = df_pop['CCA3'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "6QQyQFfkATV0",
        "outputId": "d2f1d83a-dfba-45b0-ebde-00a0692ca20d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CCA3\n",
              "AFG          Afghanistan\n",
              "ALB              Albania\n",
              "DZA              Algeria\n",
              "ASM       American Samoa\n",
              "AND              Andorra\n",
              "             ...        \n",
              "WLF    Wallis and Futuna\n",
              "ESH       Western Sahara\n",
              "YEM                Yemen\n",
              "ZMB               Zambia\n",
              "ZWE             Zimbabwe\n",
              "Name: Country/Territory, Length: 234, dtype: object"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "country_map = df_pop.loc[:, ['CCA3', 'Country/Territory']].set_index('CCA3')['Country/Territory']\n",
        "country_map.to_csv('country_map.csv')\n",
        "country_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z83Ni1WI80mT",
        "outputId": "85431d8d-e5d1-45a9-9130-619fac43f4d9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['Africa Eastern and Southern', 'Africa Western and Central',\n",
              "       'Arab World', 'Central Europe and the Baltics',\n",
              "       'Caribbean small states',\n",
              "       'East Asia & Pacific (excluding high income)',\n",
              "       'Early-demographic dividend', 'East Asia & Pacific',\n",
              "       'Europe & Central Asia (excluding high income)',\n",
              "       'Europe & Central Asia', 'Euro area', 'European Union',\n",
              "       'Fragile and conflict affected situations', 'High income',\n",
              "       'Heavily indebted poor countries (HIPC)', 'IBRD only',\n",
              "       'IDA & IBRD total', 'IDA total', 'IDA blend', 'IDA only',\n",
              "       'Latin America & Caribbean (excluding high income)',\n",
              "       'Latin America & Caribbean',\n",
              "       'Least developed countries: UN classification', 'Low income',\n",
              "       'Lower middle income', 'Low & middle income',\n",
              "       'Late-demographic dividend', 'Middle East & North Africa',\n",
              "       'Middle income',\n",
              "       'Middle East & North Africa (excluding high income)',\n",
              "       'North America', 'OECD members', 'Other small states',\n",
              "       'Pre-demographic dividend', 'Pacific island small states',\n",
              "       'Post-demographic dividend', 'South Asia',\n",
              "       'Sub-Saharan Africa (excluding high income)', 'Sub-Saharan Africa',\n",
              "       'Small states', 'East Asia & Pacific (IDA & IBRD countries)',\n",
              "       'Europe & Central Asia (IDA & IBRD countries)',\n",
              "       'Latin America & the Caribbean (IDA & IBRD countries)',\n",
              "       'Middle East & North Africa (IDA & IBRD countries)',\n",
              "       'South Asia (IDA & IBRD)',\n",
              "       'Sub-Saharan Africa (IDA & IBRD countries)', 'Upper middle income',\n",
              "       'World', 'Kosovo'], dtype=object)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "missing_countries = set(co2_countries) - set(pop_countries)\n",
        "df_co2[df_co2['country_code'].isin(missing_countries)]['country_name'].unique()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-Yurhem7tyPC"
      },
      "outputs": [],
      "source": [
        "\n",
        "df_co2 = df_co2[~df_co2['country_code'].isin(missing_countries)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "5mLrJSY3qc_r",
        "outputId": "e87921bb-4a60-4509-946b-a64945d621e4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>CCA3</th>\n",
              "      <th>population</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1970</td>\n",
              "      <td>ABW</td>\n",
              "      <td>59106.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1971</td>\n",
              "      <td>ABW</td>\n",
              "      <td>59422.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1972</td>\n",
              "      <td>ABW</td>\n",
              "      <td>59738.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1973</td>\n",
              "      <td>ABW</td>\n",
              "      <td>60054.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1974</td>\n",
              "      <td>ABW</td>\n",
              "      <td>60370.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12397</th>\n",
              "      <td>2018</td>\n",
              "      <td>ZWE</td>\n",
              "      <td>15063774.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12398</th>\n",
              "      <td>2019</td>\n",
              "      <td>ZWE</td>\n",
              "      <td>15366720.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12399</th>\n",
              "      <td>2020</td>\n",
              "      <td>ZWE</td>\n",
              "      <td>15669666.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12400</th>\n",
              "      <td>2021</td>\n",
              "      <td>ZWE</td>\n",
              "      <td>15995101.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12401</th>\n",
              "      <td>2022</td>\n",
              "      <td>ZWE</td>\n",
              "      <td>16320537.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>12402 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       year CCA3  population\n",
              "0      1970  ABW     59106.0\n",
              "1      1971  ABW     59422.1\n",
              "2      1972  ABW     59738.2\n",
              "3      1973  ABW     60054.3\n",
              "4      1974  ABW     60370.4\n",
              "...     ...  ...         ...\n",
              "12397  2018  ZWE  15063774.4\n",
              "12398  2019  ZWE  15366720.2\n",
              "12399  2020  ZWE  15669666.0\n",
              "12400  2021  ZWE  15995101.5\n",
              "12401  2022  ZWE  16320537.0\n",
              "\n",
              "[12402 rows x 3 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Step 1: Prepare df_pop in long format\n",
        "df_pop_long = df_pop.melt(\n",
        "    id_vars=['CCA3'],\n",
        "    value_vars=['1970 Population', '1980 Population', '1990 Population', '2000 Population',\n",
        "                '2010 Population', '2015 Population', '2020 Population', '2022 Population'],\n",
        "    var_name='year',\n",
        "    value_name='population'\n",
        ")\n",
        "\n",
        "# Clean and convert year to integer\n",
        "df_pop_long['year'] = df_pop_long['year'].str.extract(r'(\\d+)').astype(int)\n",
        "\n",
        "all_years = np.arange(df_pop_long['year'].min(), df_pop_long['year'].max() + 1)\n",
        "\n",
        "df_list = []\n",
        "for country, group in df_pop_long.groupby('CCA3'):\n",
        "    group = group.set_index('year').reindex(all_years)\n",
        "    group['CCA3'] = country\n",
        "    group['population'] = group['population'].interpolate(method='linear')\n",
        "    df_list.append(group.reset_index())\n",
        "\n",
        "df_pop_interp = pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "df_pop_interp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pV24KKUKq0u2"
      },
      "outputs": [],
      "source": [
        "# Step 3: Merge safely\n",
        "df_co2 = df_co2.merge(\n",
        "    df_pop_interp,\n",
        "    how='inner',\n",
        "    left_on=['country_code', 'year'],\n",
        "    right_on=['CCA3', 'year']\n",
        ").drop(columns=['CCA3'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "zhNqWLNZuNYh",
        "outputId": "cddea204-5553-4e3a-ba97-3eee82220d79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "country_code      0\n",
            "year              0\n",
            "value             0\n",
            "co2_per_capita    0\n",
            "dtype: int64\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>country_code</th>\n",
              "      <th>year</th>\n",
              "      <th>value</th>\n",
              "      <th>co2_per_capita</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ABW</td>\n",
              "      <td>1970</td>\n",
              "      <td>16655.514</td>\n",
              "      <td>0.281791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ABW</td>\n",
              "      <td>1971</td>\n",
              "      <td>14495.651</td>\n",
              "      <td>0.243944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ABW</td>\n",
              "      <td>1972</td>\n",
              "      <td>14055.611</td>\n",
              "      <td>0.235287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ABW</td>\n",
              "      <td>1973</td>\n",
              "      <td>15592.084</td>\n",
              "      <td>0.259633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ABW</td>\n",
              "      <td>1974</td>\n",
              "      <td>14132.618</td>\n",
              "      <td>0.234098</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  country_code  year      value  co2_per_capita\n",
              "0          ABW  1970  16655.514        0.281791\n",
              "1          ABW  1971  14495.651        0.243944\n",
              "2          ABW  1972  14055.611        0.235287\n",
              "3          ABW  1973  15592.084        0.259633\n",
              "4          ABW  1974  14132.618        0.234098"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_co2['co2_per_capita'] = df_co2['value'] / df_co2['population']\n",
        "df_co2 = df_co2.drop(columns=[\"country_name\", \"population\"])\n",
        "df_co2.rename(columns={'value':'co2'})\n",
        "\n",
        "df_co2.to_csv('co2.csv')\n",
        "print(df_co2.isna().sum())\n",
        "df_co2.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hj7SdoRZjEb8"
      },
      "source": [
        "## Climate DF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "by_gtDX1sa58",
        "outputId": "bbfaee0d-7d11-4c3c-ac1b-aeb3fa116417"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overlapping IDs: 3458\n",
            "     year  month IIiii   G1       Po  G1.1        P  G1.2   sn     T  ...  \\\n",
            "0  2013.0    4.0  1001  1.0  10095.0   2.0  10107.0   3.0  1.0  42.0  ...   \n",
            "1  2013.0    4.0  1007  1.0  10084.0   2.0  10094.0   3.0  1.0  92.0  ...   \n",
            "2  2013.0    4.0  1008  1.0  10056.0   2.0  10091.0   3.0  1.0  93.0  ...   \n",
            "3  2013.0    4.0  1025  1.0  10043.0   2.0  10054.0   3.0  0.0  10.0  ...   \n",
            "4  2013.0    4.0  1026  1.0   9912.0   2.0  10054.0   3.0  0.0  12.0  ...   \n",
            "\n",
            "   Dgr  G4.7  iy  Gx  Gn                      name  \\\n",
            "0  NaN   NaN NaN NaN NaN                 Jan Mayen   \n",
            "1  NaN   NaN NaN NaN NaN                Ny-Alesund   \n",
            "2  NaN   NaN NaN NaN NaN                  Svalbard   \n",
            "3  NaN   NaN NaN NaN NaN            Tromso/Langnes   \n",
            "4  NaN   NaN NaN NaN NaN                    Tromso   \n",
            "\n",
            "                                        latitude   longitude   height  \\\n",
            "0                                          70.94      -08.67        9   \n",
            "1                                          78.92       11.93        8   \n",
            "2                                          78.25       15.50       27   \n",
            "3                                          69.68       18.91        9   \n",
            "4                                          69.65       18.94      114   \n",
            "\n",
            "                                             country  \n",
            "0   Norway                                       ...  \n",
            "1   Norway                                       ...  \n",
            "2   Norway                                       ...  \n",
            "3   Norway                                       ...  \n",
            "4   Norway                                       ...  \n",
            "\n",
            "[5 rows x 132 columns]\n"
          ]
        }
      ],
      "source": [
        "# --- Step 0: Rename station columns ---\n",
        "df_stations = df_stations.rename(columns={\n",
        "    '0': 'id',\n",
        "    '1': 'name',\n",
        "    '2': 'latitude',\n",
        "    '3': 'longitude',\n",
        "    '4': 'height',\n",
        "    '5': 'country'\n",
        "})\n",
        "\n",
        "# --- Step 1: Keep only rows with non-missing, numeric IDs ---\n",
        "df_reports = df_reports[pd.to_numeric(df_reports[\"IIiii\"], errors='coerce').notna()].copy()\n",
        "df_stations = df_stations[pd.to_numeric(df_stations[\"id\"], errors='coerce').notna()].copy()\n",
        "\n",
        "# --- Step 2: Convert to int → str ---\n",
        "df_reports[\"IIiii\"] = df_reports[\"IIiii\"].astype(float).astype(int).astype(str)\n",
        "df_stations[\"id\"] = df_stations[\"id\"].astype(float).astype(int).astype(str)\n",
        "\n",
        "# --- Step 3: Check overlap ---\n",
        "reports_ids = set(df_reports[\"IIiii\"])\n",
        "stations_ids = set(df_stations[\"id\"])\n",
        "overlap_ids = reports_ids & stations_ids\n",
        "print(f\"Overlapping IDs: {len(overlap_ids)}\")\n",
        "\n",
        "# --- Step 4: Merge using inner join ---\n",
        "df_climate = pd.merge(\n",
        "    df_reports,\n",
        "    df_stations,\n",
        "    left_on=\"IIiii\",\n",
        "    right_on=\"id\",\n",
        "    how=\"inner\"\n",
        ").drop(columns=['id'])\n",
        "\n",
        "print(df_climate.head())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "KOshu5Utu8aa",
        "outputId": "029eee01-5905-40e2-dc5f-b0f20af5184d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>country</th>\n",
              "      <th>month</th>\n",
              "      <th>R1</th>\n",
              "      <th>temp_mean</th>\n",
              "      <th>temp_max</th>\n",
              "      <th>temp_min</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2003</td>\n",
              "      <td>Algeria</td>\n",
              "      <td>3</td>\n",
              "      <td>16.200000</td>\n",
              "      <td>15.023529</td>\n",
              "      <td>20.605882</td>\n",
              "      <td>9.917647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2003</td>\n",
              "      <td>Algeria</td>\n",
              "      <td>4</td>\n",
              "      <td>1025.634146</td>\n",
              "      <td>18.173171</td>\n",
              "      <td>24.068293</td>\n",
              "      <td>12.251220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2003</td>\n",
              "      <td>Algeria</td>\n",
              "      <td>5</td>\n",
              "      <td>545.631579</td>\n",
              "      <td>21.707018</td>\n",
              "      <td>27.944643</td>\n",
              "      <td>15.366667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2003</td>\n",
              "      <td>Algeria</td>\n",
              "      <td>6</td>\n",
              "      <td>1760.052632</td>\n",
              "      <td>27.694828</td>\n",
              "      <td>34.146552</td>\n",
              "      <td>21.162069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2003</td>\n",
              "      <td>Algeria</td>\n",
              "      <td>7</td>\n",
              "      <td>447.311111</td>\n",
              "      <td>30.135556</td>\n",
              "      <td>36.644444</td>\n",
              "      <td>23.735556</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   year  country  month           R1  temp_mean   temp_max   temp_min\n",
              "0  2003  Algeria      3    16.200000  15.023529  20.605882   9.917647\n",
              "1  2003  Algeria      4  1025.634146  18.173171  24.068293  12.251220\n",
              "2  2003  Algeria      5   545.631579  21.707018  27.944643  15.366667\n",
              "3  2003  Algeria      6  1760.052632  27.694828  34.146552  21.162069\n",
              "4  2003  Algeria      7   447.311111  30.135556  36.644444  23.735556"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Clean country names\n",
        "df_climate['country'] = df_climate['country'].str.replace('\\r\\n', '').str.strip()\n",
        "\n",
        "\n",
        "df_climate = df_climate.loc[:, ['country', 'year', 'month','sn', 'T', 'R1', 'sn.1', 'Tx', 'sn.2', 'Tn']]\n",
        "df_climate['temp_mean'] = (1 - 2 * df_climate['sn']) * df_climate['T'] / 10\n",
        "df_climate['temp_max']  = (1 - 2 * df_climate['sn.1']) * df_climate['Tx'] / 10\n",
        "df_climate['temp_min']  = (1 - 2 * df_climate['sn.2']) * df_climate['Tn']  / 10# assuming sn.2 also applies to Tn\n",
        "\n",
        "\n",
        "# Step 1: Compute monthly mean for each year-country-month\n",
        "df_climate = (\n",
        "    df_climate\n",
        "    .groupby(['year', 'country', 'month'], as_index=False)\n",
        "    .mean()  # averages sn, T, R1, temp\n",
        ")\n",
        "\n",
        "# Compute signed temperatures\n",
        "\n",
        "# Optional: drop original columns if no longer needed\n",
        "df_climate = df_climate.drop(columns=['T', 'Tx', 'Tn', 'sn', 'sn.1', 'sn.2'])\n",
        "\n",
        "df_climate['year'] = df_climate['year'].astype(int)\n",
        "df_climate['month'] = df_climate['month'].astype(int)\n",
        "# Check result\n",
        "df_climate.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHZYsJ8LAnZz",
        "outputId": "7f1028ab-aeb1-407c-bf8e-0542dc3be90f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'', 'Western-Sahara', 'Palau-Islands', 'Indonesien', \"People's Dem. Rep. Laos\", 'Marshall-Islands', 'Moldova, Rep. Of', 'Australien, SW-Pazifik', 'Wallis-Islands', 'Dem. Republic of the Congo', 'Macedonia', 'Syrian Arab Rep.', 'Caroline-Islands', 'Slovakia (Slovak. Rep.)', 'St. Maarten', 'Wake-Insel', 'Bosnia and Herzegowina', 'Republic of Korea', 'Republic of China, Taiwan', 'United States of America', 'Iran (Islamic Rep. of)', 'Ascencion Island', 'Tunesia', 'Croatia/Hrvatska', 'United Kingdom of Great Britain and N.-Ireland', \"Korea, Dem. People's Rep.\", 'Russian Federation', 'Cook-Island', 'Slowenia', 'United Arab. Emirates', 'Mauretania', \"Cote d'Ivoire\"}\n",
            "{'Micronesia', 'Somalia', 'Lesotho', 'Croatia', 'Estonia', 'Marshall Islands', 'Cook Islands', 'Liechtenstein', 'Saint Barthelemy', 'Eswatini', 'Antigua and Barbuda', 'Mayotte', 'Eritrea', 'Bhutan', 'United States Virgin Islands', 'Wallis and Futuna', 'Guatemala', 'Sierra Leone', 'Timor-Leste', 'North Korea', 'Djibouti', 'Monaco', 'Samoa', 'Liberia', 'Slovenia', 'Niue', 'Sint Maarten', 'San Marino', 'Panama', 'DR Congo', 'Hong Kong', 'United States', 'Vatican City', 'Russia', 'Nauru', 'Dominica', 'Iraq', 'United Kingdom', 'Tokelau', 'Saint Martin', 'Western Sahara', 'Montserrat', 'Reunion', 'Tunisia', 'South Korea', 'Lithuania', 'Taiwan', 'Burundi', 'Bosnia and Herzegovina', 'Jersey', 'Moldova', 'Isle of Man', 'Palestine', 'Tonga', 'Palau', 'Saint Kitts and Nevis', 'Saint Lucia', 'Guernsey', 'Anguilla', 'Falkland Islands', 'Saint Vincent and the Grenadines', 'Uganda', 'Mauritania', 'United Arab Emirates', 'Syria', 'Sao Tome and Principe', 'Laos', 'Andorra', 'Slovakia', 'North Macedonia', 'Aruba', 'Solomon Islands', 'Latvia', 'Iran', 'Macau', 'Northern Mariana Islands', 'Grenada', 'Ivory Coast', 'Saint Pierre and Miquelon', 'Turks and Caicos Islands', 'British Virgin Islands'}\n"
          ]
        }
      ],
      "source": [
        "print(set(df_climate['country'].unique()) - set(country_map.unique()))\n",
        "print(set(country_map.unique()) - set(df_climate['country'].unique()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "ooiChI5_BqpO",
        "outputId": "928aa27d-5557-4792-a953-57547f4b2542"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NA codes: year               0\n",
            "country          159\n",
            "month              0\n",
            "R1               210\n",
            "temp_mean        328\n",
            "temp_max        1141\n",
            "temp_min        1151\n",
            "country_code     392\n",
            "dtype: int64\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>R1</th>\n",
              "      <th>temp_mean</th>\n",
              "      <th>temp_max</th>\n",
              "      <th>temp_min</th>\n",
              "      <th>country_code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2003</td>\n",
              "      <td>3</td>\n",
              "      <td>16.200000</td>\n",
              "      <td>15.023529</td>\n",
              "      <td>20.605882</td>\n",
              "      <td>9.917647</td>\n",
              "      <td>DZA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2003</td>\n",
              "      <td>4</td>\n",
              "      <td>1025.634146</td>\n",
              "      <td>18.173171</td>\n",
              "      <td>24.068293</td>\n",
              "      <td>12.251220</td>\n",
              "      <td>DZA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2003</td>\n",
              "      <td>5</td>\n",
              "      <td>545.631579</td>\n",
              "      <td>21.707018</td>\n",
              "      <td>27.944643</td>\n",
              "      <td>15.366667</td>\n",
              "      <td>DZA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2003</td>\n",
              "      <td>6</td>\n",
              "      <td>1760.052632</td>\n",
              "      <td>27.694828</td>\n",
              "      <td>34.146552</td>\n",
              "      <td>21.162069</td>\n",
              "      <td>DZA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2003</td>\n",
              "      <td>7</td>\n",
              "      <td>447.311111</td>\n",
              "      <td>30.135556</td>\n",
              "      <td>36.644444</td>\n",
              "      <td>23.735556</td>\n",
              "      <td>DZA</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   year  month           R1  temp_mean   temp_max   temp_min country_code\n",
              "0  2003      3    16.200000  15.023529  20.605882   9.917647          DZA\n",
              "1  2003      4  1025.634146  18.173171  24.068293  12.251220          DZA\n",
              "2  2003      5   545.631579  21.707018  27.944643  15.366667          DZA\n",
              "3  2003      6  1760.052632  27.694828  34.146552  21.162069          DZA\n",
              "4  2003      7   447.311111  30.135556  36.644444  23.735556          DZA"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "country_corrections = {\n",
        "    'Australien, SW-Pazifik': 'Australia',\n",
        "    'Cook-Island': 'Cook Islands',\n",
        "    'United States of America': 'United States',\n",
        "    'Caroline-Islands': 'Micronesia',\n",
        "    'Tunesia': 'Tunisia',\n",
        "    'St. Maarten': 'Sint Maarten',\n",
        "    'Croatia/Hrvatska': 'Croatia',\n",
        "    'Republic of China, Taiwan': 'Taiwan',\n",
        "    'Slowenia': 'Slovenia',\n",
        "    'Indonesien': 'Indonesia',\n",
        "    'Palau-Islands': 'Palau',\n",
        "    \"Cote d'Ivoire\": 'Ivory Coast',\n",
        "    'Russian Federation': 'Russia',\n",
        "    \"Korea, Dem. People's Rep.\": 'North Korea',\n",
        "    'Iran (Islamic Rep. of)': 'Iran',\n",
        "    'Mauretania': 'Mauritania',\n",
        "    'Marshall-Islands': 'Marshall Islands',\n",
        "    'Macedonia': 'North Macedonia',\n",
        "    'United Arab. Emirates': 'United Arab Emirates',\n",
        "    'Republic of Korea': 'South Korea',\n",
        "    'United Kingdom of Great Britain and N.-Ireland': 'United Kingdom',\n",
        "    \"People's Dem. Rep. Laos\": 'Laos',\n",
        "    'Bosnia and Herzegowina': 'Bosnia and Herzegovina',\n",
        "    'Ascencion Island': 'Ascension Island',\n",
        "    'Syrian Arab Rep.': 'Syria',\n",
        "    'Dem. Republic of the Congo': 'DR Congo',\n",
        "    'Slovakia (Slovak. Rep.)': 'Slovakia',\n",
        "    'Western-Sahara': 'Western Sahara',\n",
        "    'Wake-Insel': 'Wake Island',\n",
        "    'Moldova, Rep. Of': 'Moldova',\n",
        "    'Wallis-Islands': 'Wallis and Futuna',\n",
        "    '': None  # optional: empty string to None\n",
        "}\n",
        "df_climate['country'] = df_climate['country'].replace(country_corrections)\n",
        "\n",
        "# country_map: index = code, values = country_name\n",
        "# Create a reversed mapping: country_name -> code\n",
        "reversed_map = pd.Series(country_map.index, index=country_map.values)\n",
        "\n",
        "# Now map the country names to codes\n",
        "df_climate['country_code'] = df_climate['country'].map(reversed_map)\n",
        "print(f\"NA codes: {df_climate.isna().sum()}\")\n",
        "\n",
        "df_climate = df_climate.drop(columns = 'country')\n",
        "df_climate = df_climate.dropna(subset=[\"country_code\", \"temp_mean\"])\n",
        "\n",
        "df_climate.to_csv('climate.csv')\n",
        "df_climate.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QFzSs7bkjSF"
      },
      "source": [
        "## AQI DFs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSqjmvC9nflH"
      },
      "source": [
        "### AQ1: Various Pollutants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfwbuNvLklCz",
        "outputId": "4da6c83f-2072-49ef-b70a-5c091fc01091"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 54255 entries, 0 to 54254\n",
            "Data columns (total 10 columns):\n",
            " #   Column         Non-Null Count  Dtype  \n",
            "---  ------         --------------  -----  \n",
            " 0   Country Code   54255 non-null  object \n",
            " 1   City           30209 non-null  object \n",
            " 2   Location       54253 non-null  object \n",
            " 3   Coordinates    54185 non-null  object \n",
            " 4   Pollutant      54255 non-null  object \n",
            " 5   Source Name    54255 non-null  object \n",
            " 6   Unit           54255 non-null  object \n",
            " 7   Value          54255 non-null  float64\n",
            " 8   Last Updated   54255 non-null  object \n",
            " 9   Country Label  54140 non-null  object \n",
            "dtypes: float64(1), object(9)\n",
            "memory usage: 4.1+ MB\n"
          ]
        }
      ],
      "source": [
        "df_aq1.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "PeSTcYLhnxPW"
      },
      "outputs": [],
      "source": [
        "units = df_aq1.groupby('Pollutant')['Unit'].agg(lambda x: x.value_counts().idxmax())\n",
        "\n",
        "df_aq1 = df_aq1[df_aq1['Unit'] == df_aq1['Pollutant'].map(units)]\n",
        "\n",
        "means_by_country = df_aq1.groupby(['Country Label', 'Pollutant', 'Unit'])['Value'].mean()\n",
        "# Collapse the MultiIndex into columns\n",
        "means_by_country = means_by_country.reset_index()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 487 entries, 0 to 486\n",
            "Data columns (total 4 columns):\n",
            " #   Column         Non-Null Count  Dtype  \n",
            "---  ------         --------------  -----  \n",
            " 0   Country Label  487 non-null    object \n",
            " 1   Pollutant      487 non-null    object \n",
            " 2   Unit           487 non-null    object \n",
            " 3   Value          487 non-null    float64\n",
            "dtypes: float64(1), object(3)\n",
            "memory usage: 15.3+ KB\n"
          ]
        }
      ],
      "source": [
        "means_by_country.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "country_label_corrections = {\n",
        "    'Russian Federation': 'Russia',\n",
        "    \"Lao People's Dem. Rep.\": 'Laos',\n",
        "    'Sudan, The Republic of': 'Sudan',\n",
        "    'Congo, Democratic Republic of the': 'DR Congo',\n",
        "    'Korea, Republic of': 'South Korea',\n",
        "    'Viet Nam': 'Vietnam',\n",
        "    'Moldova, Republic of': 'Moldova',\n",
        "    \"Côte d'Ivoire\": 'Ivory Coast',\n",
        "    'Hong Kong, China': 'Hong Kong',\n",
        "    'Taiwan, China': 'Taiwan',\n",
        "    'USSR': 'Russia',  # historical mapping\n",
        "    'Serbia and Montenegro': 'Serbia',\n",
        "    'Macedonia, The former Yugoslav Rep. of': 'North Macedonia'\n",
        "}\n",
        "means_by_country['Country Label'] = means_by_country['Country Label'].replace(country_label_corrections)\n",
        "\n",
        "# Map country names to codes\n",
        "means_by_country['country_code'] = means_by_country['Country Label'].map(reversed_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Country Label    0\n",
              "Pollutant        0\n",
              "Unit             0\n",
              "Value            0\n",
              "country_code     0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "means_by_country.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "means_by_country.drop(columns=['Country Label'], inplace=True)\n",
        "means_by_country.rename(columns={\n",
        "    'Value':'value',\n",
        "    'Pollutant':'pollutant',\n",
        "    'Unit':'unit'}, inplace=True)\n",
        "\n",
        "means_by_country.to_csv('pollution.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktHTV-4anZC5"
      },
      "source": [
        "### AQ2: AQI by country by Month"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "JWKlSG2kkn0l",
        "outputId": "a1d24afc-0d4e-4b13-8d6d-7a7905c68794"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>avg</th>\n",
              "      <th>jan</th>\n",
              "      <th>feb</th>\n",
              "      <th>mar</th>\n",
              "      <th>apr</th>\n",
              "      <th>may</th>\n",
              "      <th>jun</th>\n",
              "      <th>jul</th>\n",
              "      <th>aug</th>\n",
              "      <th>sep</th>\n",
              "      <th>oct</th>\n",
              "      <th>nov</th>\n",
              "      <th>dec</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Country</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Bangladesh</th>\n",
              "      <td>161.000000</td>\n",
              "      <td>303.000000</td>\n",
              "      <td>195.000000</td>\n",
              "      <td>184.000000</td>\n",
              "      <td>116.000000</td>\n",
              "      <td>125.000000</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>51.000000</td>\n",
              "      <td>107.000000</td>\n",
              "      <td>83.000000</td>\n",
              "      <td>158.000000</td>\n",
              "      <td>223.000000</td>\n",
              "      <td>284.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Chad</th>\n",
              "      <td>147.000000</td>\n",
              "      <td>221.000000</td>\n",
              "      <td>326.000000</td>\n",
              "      <td>189.000000</td>\n",
              "      <td>285.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>87.500000</td>\n",
              "      <td>58.500000</td>\n",
              "      <td>38.000000</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>207.000000</td>\n",
              "      <td>228.000000</td>\n",
              "      <td>235.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Kuwait</th>\n",
              "      <td>95.333333</td>\n",
              "      <td>90.000000</td>\n",
              "      <td>96.000000</td>\n",
              "      <td>72.666667</td>\n",
              "      <td>78.333333</td>\n",
              "      <td>111.333333</td>\n",
              "      <td>117.666667</td>\n",
              "      <td>92.333333</td>\n",
              "      <td>110.000000</td>\n",
              "      <td>108.666667</td>\n",
              "      <td>97.666667</td>\n",
              "      <td>81.333333</td>\n",
              "      <td>86.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>India</th>\n",
              "      <td>83.283372</td>\n",
              "      <td>132.065574</td>\n",
              "      <td>109.847775</td>\n",
              "      <td>85.391101</td>\n",
              "      <td>77.925059</td>\n",
              "      <td>67.594848</td>\n",
              "      <td>62.161593</td>\n",
              "      <td>42.587822</td>\n",
              "      <td>55.494145</td>\n",
              "      <td>48.189696</td>\n",
              "      <td>81.800937</td>\n",
              "      <td>114.119438</td>\n",
              "      <td>121.674473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Pakistan</th>\n",
              "      <td>79.166667</td>\n",
              "      <td>124.500000</td>\n",
              "      <td>82.333333</td>\n",
              "      <td>61.833333</td>\n",
              "      <td>42.666667</td>\n",
              "      <td>36.666667</td>\n",
              "      <td>41.833333</td>\n",
              "      <td>33.000000</td>\n",
              "      <td>41.222222</td>\n",
              "      <td>45.555556</td>\n",
              "      <td>77.666667</td>\n",
              "      <td>177.500000</td>\n",
              "      <td>191.666667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   avg         jan         feb         mar         apr  \\\n",
              "Country                                                                  \n",
              "Bangladesh  161.000000  303.000000  195.000000  184.000000  116.000000   \n",
              "Chad        147.000000  221.000000  326.000000  189.000000  285.000000   \n",
              "Kuwait       95.333333   90.000000   96.000000   72.666667   78.333333   \n",
              "India        83.283372  132.065574  109.847775   85.391101   77.925059   \n",
              "Pakistan     79.166667  124.500000   82.333333   61.833333   42.666667   \n",
              "\n",
              "                   may         jun        jul         aug         sep  \\\n",
              "Country                                                                 \n",
              "Bangladesh  125.000000  100.000000  51.000000  107.000000   83.000000   \n",
              "Chad        106.000000   87.500000  58.500000   38.000000   35.000000   \n",
              "Kuwait      111.333333  117.666667  92.333333  110.000000  108.666667   \n",
              "India        67.594848   62.161593  42.587822   55.494145   48.189696   \n",
              "Pakistan     36.666667   41.833333  33.000000   41.222222   45.555556   \n",
              "\n",
              "                   oct         nov         dec  \n",
              "Country                                         \n",
              "Bangladesh  158.000000  223.000000  284.000000  \n",
              "Chad        207.000000  228.000000  235.000000  \n",
              "Kuwait       97.666667   81.333333   86.666667  \n",
              "India        81.800937  114.119438  121.674473  \n",
              "Pakistan     77.666667  177.500000  191.666667  "
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cols = [c for c in df_aq2.columns if c not in ['rank', 'city', 'city_name', 'Country']]\n",
        "df_aq = df_aq2.groupby('Country')[cols].mean()\n",
        "\n",
        "df_aq.to_csv('air_quality.csv')\n",
        "df_aq.sort_values('avg', ascending=False).head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_aq = df_aq.reset_index()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Countries in df_aq not in country_map:\n",
            "{'Cabo Verde', 'Congo (Kinshasa)', 'Bosnia And Herzegovina', 'Macedonia', 'Trinidad And Tobago', 'Cote D’Ivoire', 'Burma', 'Czechia', 'Kosovo'}\n",
            "Countries in country_map not in df_aq:\n",
            "{'Micronesia', 'Suriname', 'Somalia', 'Papua New Guinea', 'Tuvalu', 'Lesotho', 'Kiribati', 'Central African Republic', 'Cook Islands', 'Marshall Islands', 'Myanmar', 'American Samoa', 'Comoros', 'Saint Barthelemy', 'Eswatini', 'Bermuda', 'Antigua and Barbuda', 'Mayotte', 'Eritrea', 'Fiji', 'Dominican Republic', 'Bhutan', 'United States Virgin Islands', 'Wallis and Futuna', 'Uruguay', 'Sierra Leone', 'Maldives', 'Tanzania', 'Timor-Leste', 'Djibouti', 'Paraguay', 'Samoa', 'Greenland', 'Oman', 'Bahamas', 'Niue', 'Equatorial Guinea', 'Mozambique', 'Sint Maarten', 'Faroe Islands', 'Panama', 'Mauritius', 'DR Congo', 'Hong Kong', 'Vatican City', 'Vanuatu', 'French Polynesia', 'Nauru', 'Nicaragua', 'Dominica', 'Republic of the Congo', 'Tokelau', 'Saint Martin', 'Niger', 'Western Sahara', 'South Sudan', 'Montserrat', 'Trinidad and Tobago', 'Tunisia', 'Zimbabwe', 'Burundi', 'Bosnia and Herzegovina', 'Guyana', 'Jersey', 'Belize', 'Isle of Man', 'Seychelles', 'Cape Verde', 'Guinea-Bissau', 'Haiti', 'Cayman Islands', 'Palestine', 'Tonga', 'Palau', 'Saint Kitts and Nevis', 'Saint Lucia', 'Guernsey', 'Malawi', 'Anguilla', 'Yemen', 'Falkland Islands', 'Saint Vincent and the Grenadines', 'Mauritania', 'Barbados', 'Syria', 'Sao Tome and Principe', 'Morocco', 'Czech Republic', 'Honduras', 'Cuba', 'North Macedonia', 'Aruba', 'Solomon Islands', 'Macau', 'Northern Mariana Islands', 'Jamaica', 'Ivory Coast', 'Saint Pierre and Miquelon', 'Turks and Caicos Islands', 'British Virgin Islands', 'Guam', 'Libya'}\n"
          ]
        }
      ],
      "source": [
        "# Countries in df_aq not in country_map\n",
        "print(\"Countries in df_aq not in country_map:\")\n",
        "print(set(df_aq['Country'].unique()) - set(country_map.unique()))\n",
        "\n",
        "# Countries in country_map not in df_aq\n",
        "print(\"Countries in country_map not in df_aq:\")\n",
        "print(set(country_map.unique()) - set(df_aq['Country'].unique()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "country_fix_aq = {\n",
        "    'Czechia': 'Czech Republic',\n",
        "    'Cabo Verde': 'Cape Verde',\n",
        "    'Trinidad And Tobago': 'Trinidad and Tobago',\n",
        "    'Cote D’Ivoire': 'Ivory Coast',\n",
        "    'Congo (Kinshasa)': 'DR Congo',\n",
        "    'Kosovo': 'XK',  # Kosovo may not be in country_map\n",
        "    'Burma': 'Myanmar',\n",
        "    'Bosnia And Herzegovina': 'Bosnia and Herzegovina',\n",
        "    'Macedonia': 'North Macedonia'\n",
        "}\n",
        "df_aq['Country'] = df_aq['Country'].replace(country_fix_aq)\n",
        "df_aq['country_code'] = df_aq['Country'].map(reversed_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Country         0\n",
              "avg             0\n",
              "jan             0\n",
              "feb             0\n",
              "mar             0\n",
              "apr             0\n",
              "may             0\n",
              "jun             0\n",
              "jul             0\n",
              "aug             0\n",
              "sep             0\n",
              "oct             0\n",
              "nov             0\n",
              "dec             0\n",
              "country_code    1\n",
              "dtype: int64"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_aq.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_aq.dropna(inplace=True)\n",
        "df_aq.drop(columns=['Country', ], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_aq.to_csv('air_quality.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imputation of countries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded datasets:\n",
            "Air Quality: (140, 14)\n",
            "Climate: (29363, 7)\n",
            "CO2: (9466, 4)\n",
            "Pollution: (487, 4)\n",
            "\n",
            "Merged dataset shape: (98, 33)\n",
            "\n",
            "Missing values in merged dataset:\n",
            "BC                  96\n",
            "NOX                 93\n",
            "NO                  93\n",
            "RELATIVEHUMIDITY    82\n",
            "UM003               82\n",
            "PM1                 81\n",
            "TEMPERATURE         70\n",
            "CO                  55\n",
            "O3                  52\n",
            "NO2                 49\n",
            "SO2                 49\n",
            "PM10                36\n",
            "temp_max             1\n",
            "PM2.5                1\n",
            "temp_min             1\n",
            "avg                  0\n",
            "jan                  0\n",
            "temp_mean            0\n",
            "R1                   0\n",
            "country_code         0\n",
            "dec                  0\n",
            "oct                  0\n",
            "nov                  0\n",
            "sep                  0\n",
            "aug                  0\n",
            "apr                  0\n",
            "jul                  0\n",
            "jun                  0\n",
            "may                  0\n",
            "feb                  0\n",
            "mar                  0\n",
            "co2_mean             0\n",
            "co2_per_capita       0\n",
            "dtype: int64\n",
            "\n",
            "================================================================================\n",
            "CORRELATION ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "Correlation matrix shape: (32, 32)\n",
            "\n",
            "Top correlations (absolute value > 0.5):\n",
            "--------------------------------------------------------------------------------\n",
            "jul                       <-> BC                        : -1.0000\n",
            "aug                       <-> BC                        : -1.0000\n",
            "oct                       <-> BC                        :  1.0000\n",
            "nov                       <-> BC                        :  1.0000\n",
            "NO                        <-> RELATIVEHUMIDITY          : -1.0000\n",
            "feb                       <-> BC                        :  1.0000\n",
            "apr                       <-> BC                        :  1.0000\n",
            "may                       <-> BC                        :  1.0000\n",
            "sep                       <-> BC                        :  1.0000\n",
            "dec                       <-> BC                        :  1.0000\n",
            "\n",
            "Test dataset shape: (50, 33)\n",
            "\n",
            "================================================================================\n",
            "REGRESSION ANALYSIS & IMPUTATION TEST RESULTS\n",
            "================================================================================\n",
            "\n",
            "Testing regression imputation on columns: ['avg', 'jan', 'feb', 'mar', 'apr']\n",
            "\n",
            "================================================================================\n",
            "Target Column: avg\n",
            "================================================================================\n",
            "\n",
            "📊 CORRELATION-BASED IMPUTATION RESULTS (10 successful runs):\n",
            "--------------------------------------------------------------------------------\n",
            "Imputation Accuracy:\n",
            "  • Mean MSE:       277.5047 (±112.8501)\n",
            "  • Mean MAE:       10.0002 (±2.2011)\n",
            "\n",
            "📊 REGRESSION-BASED IMPUTATION RESULTS (10 successful runs):\n",
            "--------------------------------------------------------------------------------\n",
            "Model Performance:\n",
            "  • Training R²:    0.9994 (±0.0003)\n",
            "  • Adjusted R²:    0.9952 (±0.0023)\n",
            "  • Test R²:        0.3207 (±0.7157)\n",
            "\n",
            "Imputation Accuracy:\n",
            "  • Mean MSE:       316.6319 (±249.1501)\n",
            "  • Mean MAE:       10.6391 (±3.8908)\n",
            "\n",
            "🔍 Top 5 Most Important Features (by absolute coefficient):\n",
            "--------------------------------------------------------------------------------\n",
            "  1. nov                       | Coef:   -13.8696 | |Coef|:  13.8696\n",
            "  2. oct                       | Coef:    12.8019 | |Coef|:  12.8019\n",
            "  3. dec                       | Coef:    11.9626 | |Coef|:  11.9626\n",
            "  4. jun                       | Coef:     8.6876 | |Coef|:   9.2884\n",
            "  5. mar                       | Coef:     5.4128 | |Coef|:   7.9671\n",
            "\n",
            "📐 Regression Model Info:\n",
            "  • Number of features: 30\n",
            "  • Training samples:   35\n",
            "  • Test samples:      15\n",
            "  • Intercept:         38.1098\n",
            "\n",
            "================================================================================\n",
            "Target Column: jan\n",
            "================================================================================\n",
            "\n",
            "📊 CORRELATION-BASED IMPUTATION RESULTS (10 successful runs):\n",
            "--------------------------------------------------------------------------------\n",
            "Imputation Accuracy:\n",
            "  • Mean MSE:       1444.1144 (±1210.3819)\n",
            "  • Mean MAE:       19.8383 (±5.8274)\n",
            "\n",
            "📊 REGRESSION-BASED IMPUTATION RESULTS (10 successful runs):\n",
            "--------------------------------------------------------------------------------\n",
            "Model Performance:\n",
            "  • Training R²:    0.9951 (±0.0035)\n",
            "  • Adjusted R²:    0.9584 (±0.0299)\n",
            "  • Test R²:        -1.4046 (±1.9134)\n",
            "\n",
            "Imputation Accuracy:\n",
            "  • Mean MSE:       3768.4457 (±2552.0627)\n",
            "  • Mean MAE:       39.6775 (±12.5456)\n",
            "\n",
            "🔍 Top 5 Most Important Features (by absolute coefficient):\n",
            "--------------------------------------------------------------------------------\n",
            "  1. avg                       | Coef:    43.7708 | |Coef|:  60.2402\n",
            "  2. nov                       | Coef:    19.8462 | |Coef|:  52.8962\n",
            "  3. jun                       | Coef:   -43.3092 | |Coef|:  43.9997\n",
            "  4. oct                       | Coef:   -33.3477 | |Coef|:  43.4745\n",
            "  5. dec                       | Coef:    22.2642 | |Coef|:  35.8651\n",
            "\n",
            "📐 Regression Model Info:\n",
            "  • Number of features: 30\n",
            "  • Training samples:   35\n",
            "  • Test samples:      15\n",
            "  • Intercept:         54.5596\n",
            "\n",
            "================================================================================\n",
            "Target Column: feb\n",
            "================================================================================\n",
            "\n",
            "📊 CORRELATION-BASED IMPUTATION RESULTS (10 successful runs):\n",
            "--------------------------------------------------------------------------------\n",
            "Imputation Accuracy:\n",
            "  • Mean MSE:       895.4466 (±714.1554)\n",
            "  • Mean MAE:       16.3396 (±3.6425)\n",
            "\n",
            "📊 REGRESSION-BASED IMPUTATION RESULTS (10 successful runs):\n",
            "--------------------------------------------------------------------------------\n",
            "Model Performance:\n",
            "  • Training R²:    0.9946 (±0.0041)\n",
            "  • Adjusted R²:    0.9540 (±0.0347)\n",
            "  • Test R²:        -4.0320 (±7.2088)\n",
            "\n",
            "Imputation Accuracy:\n",
            "  • Mean MSE:       5526.8575 (±5116.7322)\n",
            "  • Mean MAE:       42.9922 (±15.1702)\n",
            "\n",
            "🔍 Top 5 Most Important Features (by absolute coefficient):\n",
            "--------------------------------------------------------------------------------\n",
            "  1. avg                       | Coef:    61.9301 | |Coef|:  90.0867\n",
            "  2. aug                       | Coef:   -35.4110 | |Coef|:  39.4668\n",
            "  3. dec                       | Coef:   -29.1407 | |Coef|:  36.8010\n",
            "  4. nov                       | Coef:     8.9980 | |Coef|:  33.1138\n",
            "  5. oct                       | Coef:   -12.1480 | |Coef|:  32.4584\n",
            "\n",
            "📐 Regression Model Info:\n",
            "  • Number of features: 30\n",
            "  • Training samples:   35\n",
            "  • Test samples:      15\n",
            "  • Intercept:         55.8261\n",
            "\n",
            "================================================================================\n",
            "Target Column: mar\n",
            "================================================================================\n",
            "\n",
            "📊 CORRELATION-BASED IMPUTATION RESULTS (10 successful runs):\n",
            "--------------------------------------------------------------------------------\n",
            "Imputation Accuracy:\n",
            "  • Mean MSE:       754.3423 (±680.3629)\n",
            "  • Mean MAE:       13.1295 (±3.7424)\n",
            "\n",
            "📊 REGRESSION-BASED IMPUTATION RESULTS (10 successful runs):\n",
            "--------------------------------------------------------------------------------\n",
            "Model Performance:\n",
            "  • Training R²:    0.9959 (±0.0021)\n",
            "  • Adjusted R²:    0.9651 (±0.0180)\n",
            "  • Test R²:        -2.6862 (±4.9314)\n",
            "\n",
            "Imputation Accuracy:\n",
            "  • Mean MSE:       2573.1250 (±2703.7352)\n",
            "  • Mean MAE:       28.3002 (±11.1372)\n",
            "\n",
            "🔍 Top 5 Most Important Features (by absolute coefficient):\n",
            "--------------------------------------------------------------------------------\n",
            "  1. avg                       | Coef:    45.4801 | |Coef|:  66.0960\n",
            "  2. oct                       | Coef:   -34.1505 | |Coef|:  39.5440\n",
            "  3. nov                       | Coef:    29.7336 | |Coef|:  39.3452\n",
            "  4. aug                       | Coef:    26.7928 | |Coef|:  26.7928\n",
            "  5. dec                       | Coef:   -11.6705 | |Coef|:  26.5626\n",
            "\n",
            "📐 Regression Model Info:\n",
            "  • Number of features: 30\n",
            "  • Training samples:   35\n",
            "  • Test samples:      15\n",
            "  • Intercept:         45.2966\n",
            "\n",
            "================================================================================\n",
            "Target Column: apr\n",
            "================================================================================\n",
            "\n",
            "📊 CORRELATION-BASED IMPUTATION RESULTS (10 successful runs):\n",
            "--------------------------------------------------------------------------------\n",
            "Imputation Accuracy:\n",
            "  • Mean MSE:       645.0704 (±577.7460)\n",
            "  • Mean MAE:       12.3672 (±2.7492)\n",
            "\n",
            "📊 REGRESSION-BASED IMPUTATION RESULTS (10 successful runs):\n",
            "--------------------------------------------------------------------------------\n",
            "Model Performance:\n",
            "  • Training R²:    0.9957 (±0.0035)\n",
            "  • Adjusted R²:    0.9636 (±0.0296)\n",
            "  • Test R²:        -1.8769 (±4.3055)\n",
            "\n",
            "Imputation Accuracy:\n",
            "  • Mean MSE:       2042.0494 (±2446.6538)\n",
            "  • Mean MAE:       25.9656 (±11.2053)\n",
            "\n",
            "🔍 Top 5 Most Important Features (by absolute coefficient):\n",
            "--------------------------------------------------------------------------------\n",
            "  1. avg                       | Coef:   -27.4797 | |Coef|:  43.9343\n",
            "  2. oct                       | Coef:    28.5646 | |Coef|:  29.1286\n",
            "  3. aug                       | Coef:   -18.3103 | |Coef|:  26.1047\n",
            "  4. jun                       | Coef:     8.2382 | |Coef|:  22.4031\n",
            "  5. mar                       | Coef:    20.2785 | |Coef|:  20.2785\n",
            "\n",
            "📐 Regression Model Info:\n",
            "  • Number of features: 30\n",
            "  • Training samples:   35\n",
            "  • Test samples:      15\n",
            "  • Intercept:         41.8095\n",
            "\n",
            "================================================================================\n",
            "SUMMARY - Mean Average MSE and MAE across all columns (10 iterations)\n",
            "================================================================================\n",
            "\n",
            "📈 Overall Regression Performance:\n",
            "  • Mean Training R²: 0.9961\n",
            "  • Mean Test R²:     -1.9358\n",
            "\n",
            "📋 Detailed results by column:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Column               | Train R²   | Test R²    | Reg MSE      | Reg MAE      | Corr MSE     | Corr MAE    \n",
            "----------------------------------------------------------------------------------------------------\n",
            "avg                  |    0.9994 |    0.3207 |    316.6319 |     10.6391 |     277.5047 |      10.0002\n",
            "jan                  |    0.9951 |   -1.4046 |   3768.4457 |     39.6775 |    1444.1144 |      19.8383\n",
            "feb                  |    0.9946 |   -4.0320 |   5526.8575 |     42.9922 |     895.4466 |      16.3396\n",
            "mar                  |    0.9959 |   -2.6862 |   2573.1250 |     28.3002 |     754.3423 |      13.1295\n",
            "apr                  |    0.9957 |   -1.8769 |   2042.0494 |     25.9656 |     645.0704 |      12.3672\n",
            "\n",
            "================================================================================\n",
            "FINAL RESULTS SUMMARY\n",
            "================================================================================\n",
            "\n",
            "🔗 CORRELATION-BASED IMPUTATION:\n",
            "  • Mean Squared Error (MSE): 803.2957\n",
            "  • Mean Absolute Error (MAE): 14.3350\n",
            "\n",
            "📊 REGRESSION-BASED IMPUTATION:\n",
            "  • Mean Squared Error (MSE): 2845.4219\n",
            "  • Mean Absolute Error (MAE): 29.5149\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load all datasets\n",
        "air_quality = pd.read_csv('data/air_quality.csv')\n",
        "climate = pd.read_csv('data/climate.csv')\n",
        "co2 = pd.read_csv('data/co2.csv')\n",
        "pollution = pd.read_csv('data/pollution.csv')\n",
        "\n",
        "print(\"Loaded datasets:\")\n",
        "print(f\"Air Quality: {air_quality.shape}\")\n",
        "print(f\"Climate: {climate.shape}\")\n",
        "print(f\"CO2: {co2.shape}\")\n",
        "print(f\"Pollution: {pollution.shape}\")\n",
        "\n",
        "# Prepare pollution data - pivot to have pollutants as columns\n",
        "pollution_pivot = pollution.pivot_table(\n",
        "    index='country_code', \n",
        "    columns='pollutant', \n",
        "    values='value', \n",
        "    aggfunc='mean'\n",
        ").reset_index()\n",
        "\n",
        "# Merge datasets on country_code\n",
        "# Start with air_quality as base\n",
        "merged = air_quality.copy()\n",
        "\n",
        "# Aggregate climate data by country (average across all years/months)\n",
        "climate_agg = climate.groupby('country_code').agg({\n",
        "    'R1': 'mean',\n",
        "    'temp_mean': 'mean',\n",
        "    'temp_max': 'mean',\n",
        "    'temp_min': 'mean'\n",
        "}).reset_index()\n",
        "\n",
        "# Aggregate CO2 data by country (average across all years)\n",
        "co2_agg = co2.groupby('country_code').agg({\n",
        "    'value': 'mean',\n",
        "    'co2_per_capita': 'mean'\n",
        "}).rename(columns={'value': 'co2_mean'}).reset_index()\n",
        "\n",
        "# Merge all datasets\n",
        "merged = merged.merge(climate_agg, on='country_code', how='inner')\n",
        "merged = merged.merge(co2_agg, on='country_code', how='inner')\n",
        "merged = merged.merge(pollution_pivot, on='country_code', how='inner')\n",
        "\n",
        "print(f\"\\nMerged dataset shape: {merged.shape}\")\n",
        "print(f\"\\nMissing values in merged dataset:\")\n",
        "print(merged.isna().sum().sort_values(ascending=False))\n",
        "\n",
        "# Select columns with numeric data for imputation\n",
        "numeric_cols = merged.select_dtypes(include=[np.number]).columns.tolist()\n",
        "# Remove country_code if it's numeric (shouldn't be used as feature)\n",
        "if 'country_code' in numeric_cols:\n",
        "    numeric_cols.remove('country_code')\n",
        "\n",
        "# ============================================================================\n",
        "# CORRELATION ANALYSIS\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CORRELATION ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Calculate correlation matrix for numeric columns\n",
        "corr_data = merged[numeric_cols].copy()\n",
        "\n",
        "# Calculate pairwise correlations\n",
        "correlation_matrix = corr_data.corr()\n",
        "\n",
        "print(f\"\\nCorrelation matrix shape: {correlation_matrix.shape}\")\n",
        "print(\"\\nTop correlations (absolute value > 0.5):\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Find top correlations (excluding self-correlations)\n",
        "top_correlations = []\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i+1, len(correlation_matrix.columns)):\n",
        "        col1 = correlation_matrix.columns[i]\n",
        "        col2 = correlation_matrix.columns[j]\n",
        "        corr_val = correlation_matrix.iloc[i, j]\n",
        "        if not np.isnan(corr_val) and abs(corr_val) > 0.5:\n",
        "            top_correlations.append((col1, col2, corr_val))\n",
        "\n",
        "# Sort by absolute correlation value\n",
        "top_correlations.sort(key=lambda x: abs(x[2]), reverse=True)\n",
        "\n",
        "# Display top 10 correlations\n",
        "for col1, col2, corr_val in top_correlations[:10]:\n",
        "    print(f\"{col1:25s} <-> {col2:25s} : {corr_val:7.4f}\")\n",
        "\n",
        "if len(top_correlations) == 0:\n",
        "    print(\"No strong correlations found (|r| > 0.5)\")\n",
        "\n",
        "# Create a small test dataset (select first 50 rows with sufficient non-missing data)\n",
        "test_data = merged.dropna(subset=numeric_cols[:5]).head(50).copy()\n",
        "print(f\"\\nTest dataset shape: {test_data.shape}\")\n",
        "\n",
        "# Function to perform correlation-based imputation\n",
        "def impute_correlation(data, target_col, test_ratio=0.3, random_state=None):\n",
        "    \"\"\"\n",
        "    Artificially introduce missing values, impute them using correlation with most correlated variable, and evaluate\n",
        "    Returns: (mse, mae) for correlation-based imputation\n",
        "    \"\"\"\n",
        "    np.random.seed(random_state)\n",
        "    \n",
        "    # Copy data\n",
        "    test_df = data.copy()\n",
        "    \n",
        "    # Select rows where target column is not missing\n",
        "    valid_mask = ~test_df[target_col].isna()\n",
        "    valid_indices = test_df[valid_mask].index\n",
        "    \n",
        "    if len(valid_indices) == 0:\n",
        "        return None, None\n",
        "    \n",
        "    # Randomly select indices to make missing\n",
        "    n_missing = max(1, int(len(valid_indices) * test_ratio))\n",
        "    missing_indices = np.random.choice(valid_indices, n_missing, replace=False)\n",
        "    \n",
        "    # Store true values\n",
        "    true_values = test_df.loc[missing_indices, target_col].copy()\n",
        "    \n",
        "    # Make values missing\n",
        "    test_df.loc[missing_indices, target_col] = np.nan\n",
        "    \n",
        "    # Prepare features (all other numeric columns)\n",
        "    feature_cols = [col for col in numeric_cols if col != target_col]\n",
        "    \n",
        "    if len(feature_cols) == 0:\n",
        "        return None, None\n",
        "    \n",
        "    # Work with data, fill missing features with median\n",
        "    test_df_imputed = test_df.copy()\n",
        "    for col in feature_cols:\n",
        "        median_val = test_df[col].median()\n",
        "        if pd.notna(median_val):\n",
        "            test_df_imputed[col] = test_df_imputed[col].fillna(median_val)\n",
        "    \n",
        "    # Get training data (where target is not missing)\n",
        "    train_mask = ~test_df_imputed[target_col].isna()\n",
        "    train_data = test_df_imputed[train_mask].copy()\n",
        "    \n",
        "    # Get test data (where target is missing)\n",
        "    test_missing = test_df_imputed[test_df_imputed.index.isin(missing_indices)].copy()\n",
        "    \n",
        "    if len(train_data) < 2 or len(test_missing) == 0:\n",
        "        return None, None\n",
        "    \n",
        "    # Calculate correlation between target and all features in training data\n",
        "    correlations = {}\n",
        "    for col in feature_cols:\n",
        "        if col in train_data.columns:\n",
        "            # Get rows where both target and feature are not NaN\n",
        "            both_notna = train_data[[target_col, col]].dropna()\n",
        "            if len(both_notna) >= 2:\n",
        "                corr = both_notna[target_col].corr(both_notna[col])\n",
        "                if pd.notna(corr):\n",
        "                    correlations[col] = abs(corr)\n",
        "    \n",
        "    if len(correlations) == 0:\n",
        "        return None, None\n",
        "    \n",
        "    # Find the most correlated feature\n",
        "    most_correlated = max(correlations, key=correlations.get)\n",
        "    \n",
        "    # Use linear relationship between target and most correlated feature\n",
        "    # Fit simple linear regression: target = a * feature + b\n",
        "    both_notna = train_data[[target_col, most_correlated]].dropna()\n",
        "    if len(both_notna) < 2:\n",
        "        return None, None\n",
        "    \n",
        "    # Calculate slope and intercept\n",
        "    x = both_notna[most_correlated].values\n",
        "    y = both_notna[target_col].values\n",
        "    \n",
        "    # Simple linear regression: y = ax + b\n",
        "    slope = np.cov(x, y)[0, 1] / np.var(x) if np.var(x) > 0 else 0\n",
        "    intercept = np.mean(y) - slope * np.mean(x)\n",
        "    \n",
        "    # Impute missing values using: target = slope * feature + intercept\n",
        "    test_x = test_missing[most_correlated].values\n",
        "    predicted = slope * test_x + intercept\n",
        "    \n",
        "    # Get true values\n",
        "    true_vals_list = []\n",
        "    valid_indices_list = []\n",
        "    \n",
        "    for i, idx in enumerate(test_missing.index):\n",
        "        if idx in true_values.index:\n",
        "            true_vals_list.append(true_values.loc[idx])\n",
        "            valid_indices_list.append(i)\n",
        "    \n",
        "    if len(true_vals_list) == 0:\n",
        "        return None, None\n",
        "    \n",
        "    true_vals = np.array(true_vals_list)\n",
        "    predicted = predicted[valid_indices_list]\n",
        "    \n",
        "    # Calculate metrics\n",
        "    if len(predicted) > 0 and len(true_vals) > 0:\n",
        "        mse = mean_squared_error(true_vals, predicted)\n",
        "        mae = mean_absolute_error(true_vals, predicted)\n",
        "        return mse, mae\n",
        "    \n",
        "    return None, None\n",
        "\n",
        "# Function to perform imputation and evaluation with regression analysis\n",
        "def impute_and_evaluate(data, target_col, test_ratio=0.3, random_state=None):\n",
        "    \"\"\"\n",
        "    Artificially introduce missing values, impute them using regression analysis, and evaluate\n",
        "    Returns: (mse, mae, regression_stats) where regression_stats includes R², coefficients, etc.\n",
        "    \"\"\"\n",
        "    from sklearn.metrics import r2_score\n",
        "    \n",
        "    np.random.seed(random_state)\n",
        "    \n",
        "    # Copy data\n",
        "    test_df = data.copy()\n",
        "    \n",
        "    # Select rows where target column is not missing\n",
        "    valid_mask = ~test_df[target_col].isna()\n",
        "    valid_indices = test_df[valid_mask].index\n",
        "    \n",
        "    if len(valid_indices) == 0:\n",
        "        return None, None, None\n",
        "    \n",
        "    # Randomly select indices to make missing\n",
        "    n_missing = max(1, int(len(valid_indices) * test_ratio))\n",
        "    missing_indices = np.random.choice(valid_indices, n_missing, replace=False)\n",
        "    \n",
        "    # Store true values\n",
        "    true_values = test_df.loc[missing_indices, target_col].copy()\n",
        "    \n",
        "    # Make values missing\n",
        "    test_df.loc[missing_indices, target_col] = np.nan\n",
        "    \n",
        "    # Prepare features (all other numeric columns)\n",
        "    feature_cols = [col for col in numeric_cols if col != target_col]\n",
        "    \n",
        "    if len(feature_cols) == 0:\n",
        "        return None, None, None\n",
        "    \n",
        "    # Work with all data, but fill missing features with median\n",
        "    # This allows us to use more data points\n",
        "    test_df_imputed = test_df.copy()\n",
        "    \n",
        "    # For each feature column, fill missing values with median\n",
        "    for col in feature_cols:\n",
        "        median_val = test_df[col].median()\n",
        "        if pd.notna(median_val):\n",
        "            test_df_imputed[col] = test_df_imputed[col].fillna(median_val)\n",
        "    \n",
        "    # Separate into train (with target) and test (missing target)\n",
        "    # Train: rows where target is NOT missing\n",
        "    train_mask = ~test_df_imputed[target_col].isna()\n",
        "    train_data = test_df_imputed[train_mask].copy()\n",
        "    \n",
        "    # Test: rows where target IS missing (the ones we artificially made missing)\n",
        "    test_missing = test_df_imputed[test_df_imputed.index.isin(missing_indices)].copy()\n",
        "    \n",
        "    # Ensure we have enough training data and test data\n",
        "    if len(train_data) < 2:\n",
        "        return None, None, None\n",
        "    \n",
        "    if len(test_missing) == 0:\n",
        "        # If test_missing is empty, try to find any rows with missing target\n",
        "        test_missing = test_df_imputed[test_df_imputed[target_col].isna()].copy()\n",
        "        if len(test_missing) == 0:\n",
        "            return None, None, None\n",
        "    \n",
        "    # Prepare X and y (features are already imputed, but ensure no remaining NaN)\n",
        "    X_train = train_data[feature_cols].copy()\n",
        "    X_test = test_missing[feature_cols].copy()\n",
        "    \n",
        "    # Remove columns that are entirely NaN\n",
        "    valid_features = []\n",
        "    for col in feature_cols:\n",
        "        if col in X_train.columns:\n",
        "            # Check if column has any valid values in training data\n",
        "            if X_train[col].notna().sum() > 0:\n",
        "                valid_features.append(col)\n",
        "    \n",
        "    if len(valid_features) == 0:\n",
        "        return None, None, None\n",
        "    \n",
        "    X_train = X_train[valid_features].copy()\n",
        "    X_test = X_test[valid_features].copy()\n",
        "    \n",
        "    # Fill remaining NaN values with column median, or 0 if median is NaN\n",
        "    for col in valid_features:\n",
        "        median_val = X_train[col].median()\n",
        "        if pd.isna(median_val):\n",
        "            # If median is NaN, use mean, or 0 if that's also NaN\n",
        "            mean_val = X_train[col].mean()\n",
        "            fill_val = mean_val if pd.notna(mean_val) else 0.0\n",
        "        else:\n",
        "            fill_val = median_val\n",
        "        \n",
        "        X_train[col] = X_train[col].fillna(fill_val)\n",
        "        X_test[col] = X_test[col].fillna(fill_val)\n",
        "    \n",
        "    # Final check: replace any remaining NaN with 0\n",
        "    X_train = X_train.fillna(0)\n",
        "    X_test = X_test.fillna(0)\n",
        "    \n",
        "    # Convert to numpy arrays and check for any remaining NaN\n",
        "    X_train_array = X_train.values\n",
        "    X_test_array = X_test.values\n",
        "    \n",
        "    if np.isnan(X_train_array).any():\n",
        "        # Replace any remaining NaN with 0\n",
        "        X_train_array = np.nan_to_num(X_train_array, nan=0.0)\n",
        "    \n",
        "    if np.isnan(X_test_array).any():\n",
        "        # Replace any remaining NaN with 0\n",
        "        X_test_array = np.nan_to_num(X_test_array, nan=0.0)\n",
        "    \n",
        "    y_train = train_data[target_col].values\n",
        "    \n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train_array)\n",
        "    X_test_scaled = scaler.transform(X_test_array)\n",
        "    \n",
        "    # Final check for NaN after scaling (shouldn't happen but just in case)\n",
        "    if np.isnan(X_train_scaled).any():\n",
        "        X_train_scaled = np.nan_to_num(X_train_scaled, nan=0.0)\n",
        "    if np.isnan(X_test_scaled).any():\n",
        "        X_test_scaled = np.nan_to_num(X_test_scaled, nan=0.0)\n",
        "    \n",
        "    # Train linear regression model\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    \n",
        "    # Regression Analysis on Training Data\n",
        "    y_train_pred = model.predict(X_train_scaled)\n",
        "    train_r2 = r2_score(y_train, y_train_pred)\n",
        "    \n",
        "    # Adjusted R-squared\n",
        "    n_train = len(y_train)\n",
        "    p = len(valid_features)  # number of features (after filtering)\n",
        "    adj_r2_train = 1 - (1 - train_r2) * (n_train - 1) / (n_train - p - 1) if (n_train - p - 1) > 0 else train_r2\n",
        "    \n",
        "    # Get regression coefficients\n",
        "    coefficients = model.coef_\n",
        "    intercept = model.intercept_\n",
        "    \n",
        "    # Feature importance (absolute value of coefficients)\n",
        "    # Use valid_features since we may have filtered out some columns\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': valid_features,\n",
        "        'coefficient': coefficients,\n",
        "        'abs_coefficient': np.abs(coefficients)\n",
        "    }).sort_values('abs_coefficient', ascending=False)\n",
        "    \n",
        "    # Predict missing values\n",
        "    predicted = model.predict(X_test_scaled)\n",
        "    \n",
        "    # Get true values for evaluation\n",
        "    # Use the stored true values (from before we made them missing)\n",
        "    true_vals_list = []\n",
        "    valid_indices = []\n",
        "    \n",
        "    for i, idx in enumerate(test_missing.index):\n",
        "        if idx in true_values.index:\n",
        "            true_vals_list.append(true_values.loc[idx])\n",
        "            valid_indices.append(i)\n",
        "        else:\n",
        "            # Fallback: try to get from original data (shouldn't happen but just in case)\n",
        "            val = true_values.get(idx, None)\n",
        "            if val is not None and pd.notna(val):\n",
        "                true_vals_list.append(val)\n",
        "                valid_indices.append(i)\n",
        "    \n",
        "    # Convert to numpy array\n",
        "    true_vals = np.array(true_vals_list)\n",
        "    predicted = predicted[valid_indices]\n",
        "    \n",
        "    if len(true_vals) == 0 or len(predicted) == 0:\n",
        "        return None, None, None\n",
        "    \n",
        "    # Calculate metrics\n",
        "    if len(predicted) > 0 and len(true_vals) > 0:\n",
        "        mse = mean_squared_error(true_vals, predicted)\n",
        "        mae = mean_absolute_error(true_vals, predicted)\n",
        "        test_r2 = r2_score(true_vals, predicted)\n",
        "        \n",
        "        # Regression statistics\n",
        "        regression_stats = {\n",
        "            'train_r2': train_r2,\n",
        "            'train_adj_r2': adj_r2_train,\n",
        "            'test_r2': test_r2,\n",
        "            'coefficients': feature_importance,\n",
        "            'intercept': intercept,\n",
        "            'n_features': len(valid_features),\n",
        "            'n_train': n_train,\n",
        "            'n_test': len(true_vals)\n",
        "        }\n",
        "        \n",
        "        return mse, mae, regression_stats\n",
        "    \n",
        "    return None, None, None\n",
        "\n",
        "# ============================================================================\n",
        "# REGRESSION ANALYSIS & IMPUTATION TEST\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"REGRESSION ANALYSIS & IMPUTATION TEST RESULTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Select a few target columns to test (columns with reasonable data)\n",
        "target_columns = []\n",
        "for col in numeric_cols[:10]:  # Test first 10 numeric columns\n",
        "    non_missing = merged[col].notna().sum()\n",
        "    if non_missing >= 20:  # At least 20 non-missing values\n",
        "        target_columns.append(col)\n",
        "\n",
        "if len(target_columns) == 0:\n",
        "    # Fallback: use columns that exist\n",
        "    target_columns = numeric_cols[:5]\n",
        "\n",
        "print(f\"\\nTesting regression imputation on columns: {target_columns[:5]}\")\n",
        "\n",
        "results_all = {}\n",
        "regression_analyses = {}\n",
        "\n",
        "for target_col in target_columns[:5]:  # Test up to 5 columns\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Target Column: {target_col}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    # Correlation-based imputation\n",
        "    corr_mse_list = []\n",
        "    corr_mae_list = []\n",
        "    \n",
        "    # Regression-based imputation\n",
        "    reg_mse_list = []\n",
        "    reg_mae_list = []\n",
        "    train_r2_list = []\n",
        "    test_r2_list = []\n",
        "    adj_r2_list = []\n",
        "    all_coefficients = []\n",
        "    all_regression_stats = []\n",
        "    \n",
        "    # Run 10 iterations for both methods\n",
        "    for i in range(10):\n",
        "        # Correlation-based imputation\n",
        "        corr_mse, corr_mae = impute_correlation(test_data, target_col, test_ratio=0.3, random_state=i)\n",
        "        if corr_mse is not None and corr_mae is not None:\n",
        "            corr_mse_list.append(corr_mse)\n",
        "            corr_mae_list.append(corr_mae)\n",
        "        \n",
        "        # Regression-based imputation\n",
        "        mse, mae, reg_stats = impute_and_evaluate(test_data, target_col, test_ratio=0.3, random_state=i)\n",
        "        if mse is not None and mae is not None and reg_stats is not None:\n",
        "            reg_mse_list.append(mse)\n",
        "            reg_mae_list.append(mae)\n",
        "            train_r2_list.append(reg_stats['train_r2'])\n",
        "            test_r2_list.append(reg_stats['test_r2'])\n",
        "            adj_r2_list.append(reg_stats['train_adj_r2'])\n",
        "            all_coefficients.append(reg_stats['coefficients'])\n",
        "            all_regression_stats.append(reg_stats)\n",
        "    \n",
        "    # Correlation-based imputation results\n",
        "    if len(corr_mse_list) > 0:\n",
        "        corr_mean_mse = np.mean(corr_mse_list)\n",
        "        corr_mean_mae = np.mean(corr_mae_list)\n",
        "        corr_std_mse = np.std(corr_mse_list)\n",
        "        corr_std_mae = np.std(corr_mae_list)\n",
        "        \n",
        "        print(f\"\\n📊 CORRELATION-BASED IMPUTATION RESULTS ({len(corr_mse_list)} successful runs):\")\n",
        "        print(\"-\" * 80)\n",
        "        print(f\"Imputation Accuracy:\")\n",
        "        print(f\"  • Mean MSE:       {corr_mean_mse:.4f} (±{corr_std_mse:.4f})\")\n",
        "        print(f\"  • Mean MAE:       {corr_mean_mae:.4f} (±{corr_std_mae:.4f})\")\n",
        "    else:\n",
        "        corr_mean_mse = None\n",
        "        corr_mean_mae = None\n",
        "        print(f\"\\n❌ Correlation-based imputation failed for {target_col}\")\n",
        "    \n",
        "    # Regression-based imputation results\n",
        "    if len(reg_mse_list) > 0:\n",
        "        reg_mean_mse = np.mean(reg_mse_list)\n",
        "        reg_mean_mae = np.mean(reg_mae_list)\n",
        "        reg_std_mse = np.std(reg_mse_list)\n",
        "        reg_std_mae = np.std(reg_mae_list)\n",
        "        mean_train_r2 = np.mean(train_r2_list)\n",
        "        mean_test_r2 = np.mean(test_r2_list)\n",
        "        mean_adj_r2 = np.mean(adj_r2_list)\n",
        "        \n",
        "        results_all[target_col] = {\n",
        "            'corr_mean_mse': corr_mean_mse,\n",
        "            'corr_mean_mae': corr_mean_mae,\n",
        "            'reg_mean_mse': reg_mean_mse,\n",
        "            'reg_mean_mae': reg_mean_mae,\n",
        "            'reg_std_mse': reg_std_mse,\n",
        "            'reg_std_mae': reg_std_mae,\n",
        "            'mean_train_r2': mean_train_r2,\n",
        "            'mean_test_r2': mean_test_r2,\n",
        "            'mean_adj_r2': mean_adj_r2,\n",
        "            'n_runs': len(reg_mse_list)\n",
        "        }\n",
        "        \n",
        "        # Average feature coefficients across all runs\n",
        "        if len(all_coefficients) > 0:\n",
        "            avg_coefs = pd.concat(all_coefficients).groupby('feature').agg({\n",
        "                'coefficient': 'mean',\n",
        "                'abs_coefficient': 'mean'\n",
        "            }).sort_values('abs_coefficient', ascending=False).reset_index()\n",
        "        \n",
        "        regression_analyses[target_col] = {\n",
        "            'avg_coefficients': avg_coefs,\n",
        "            'sample_stats': all_regression_stats[0]  # Use first run as sample\n",
        "        }\n",
        "        \n",
        "        print(f\"\\n📊 REGRESSION-BASED IMPUTATION RESULTS ({len(reg_mse_list)} successful runs):\")\n",
        "        print(\"-\" * 80)\n",
        "        print(f\"Model Performance:\")\n",
        "        print(f\"  • Training R²:    {mean_train_r2:.4f} (±{np.std(train_r2_list):.4f})\")\n",
        "        print(f\"  • Adjusted R²:    {mean_adj_r2:.4f} (±{np.std(adj_r2_list):.4f})\")\n",
        "        print(f\"  • Test R²:        {mean_test_r2:.4f} (±{np.std(test_r2_list):.4f})\")\n",
        "        print(f\"\\nImputation Accuracy:\")\n",
        "        print(f\"  • Mean MSE:       {reg_mean_mse:.4f} (±{reg_std_mse:.4f})\")\n",
        "        print(f\"  • Mean MAE:       {reg_mean_mae:.4f} (±{reg_std_mae:.4f})\")\n",
        "        \n",
        "        # Display top features (coefficients)\n",
        "        print(f\"\\n🔍 Top 5 Most Important Features (by absolute coefficient):\")\n",
        "        print(\"-\" * 80)\n",
        "        top_features = avg_coefs.head(5)\n",
        "        for idx, row in top_features.iterrows():\n",
        "            print(f\"  {idx+1}. {row['feature']:25s} | Coef: {row['coefficient']:10.4f} | |Coef|: {row['abs_coefficient']:8.4f}\")\n",
        "        \n",
        "        # Show regression equation info\n",
        "        sample = all_regression_stats[0]\n",
        "        print(f\"\\n📐 Regression Model Info:\")\n",
        "        print(f\"  • Number of features: {sample['n_features']}\")\n",
        "        print(f\"  • Training samples:   {sample['n_train']}\")\n",
        "        print(f\"  • Test samples:      {sample['n_test']}\")\n",
        "        print(f\"  • Intercept:         {sample['intercept']:.4f}\")\n",
        "    else:\n",
        "        print(f\"  ❌ Could not perform regression imputation (insufficient data)\")\n",
        "\n",
        "# Summary\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SUMMARY - Mean Average MSE and MAE across all columns (10 iterations)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if len(results_all) > 0:\n",
        "    # Correlation-based imputation results\n",
        "    all_corr_mse = [r['corr_mean_mse'] for r in results_all.values() if r['corr_mean_mse'] is not None]\n",
        "    all_corr_mae = [r['corr_mean_mae'] for r in results_all.values() if r['corr_mean_mae'] is not None]\n",
        "    \n",
        "    # Regression-based imputation results\n",
        "    all_reg_mse = [r['reg_mean_mse'] for r in results_all.values()]\n",
        "    all_reg_mae = [r['reg_mean_mae'] for r in results_all.values()]\n",
        "    all_train_r2 = [r['mean_train_r2'] for r in results_all.values()]\n",
        "    all_test_r2 = [r['mean_test_r2'] for r in results_all.values()]\n",
        "    \n",
        "    overall_corr_mse = np.mean(all_corr_mse) if len(all_corr_mse) > 0 else None\n",
        "    overall_corr_mae = np.mean(all_corr_mae) if len(all_corr_mae) > 0 else None\n",
        "    overall_reg_mse = np.mean(all_reg_mse)\n",
        "    overall_reg_mae = np.mean(all_reg_mae)\n",
        "    overall_mean_train_r2 = np.mean(all_train_r2)\n",
        "    overall_mean_test_r2 = np.mean(all_test_r2)\n",
        "    \n",
        "    print(f\"\\n📈 Overall Regression Performance:\")\n",
        "    print(f\"  • Mean Training R²: {overall_mean_train_r2:.4f}\")\n",
        "    print(f\"  • Mean Test R²:     {overall_mean_test_r2:.4f}\")\n",
        "    \n",
        "    print(\"\\n📋 Detailed results by column:\")\n",
        "    print(\"-\" * 100)\n",
        "    print(f\"{'Column':<20s} | {'Train R²':<10s} | {'Test R²':<10s} | {'Reg MSE':<12s} | {'Reg MAE':<12s} | {'Corr MSE':<12s} | {'Corr MAE':<12s}\")\n",
        "    print(\"-\" * 100)\n",
        "    for col, res in results_all.items():\n",
        "        corr_mse_str = f\"{res['corr_mean_mse']:>11.4f}\" if res['corr_mean_mse'] is not None else \"N/A\"\n",
        "        corr_mae_str = f\"{res['corr_mean_mae']:>11.4f}\" if res['corr_mean_mae'] is not None else \"N/A\"\n",
        "        print(f\"{col:<20s} | {res['mean_train_r2']:>9.4f} | {res['mean_test_r2']:>9.4f} | \"\n",
        "              f\"{res['reg_mean_mse']:>11.4f} | {res['reg_mean_mae']:>11.4f} | {corr_mse_str:>12s} | {corr_mae_str:>12s}\")\n",
        "    \n",
        "    # Final summary text for MSE and MAE\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"FINAL RESULTS SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\n🔗 CORRELATION-BASED IMPUTATION:\")\n",
        "    if overall_corr_mse is not None:\n",
        "        print(f\"  • Mean Squared Error (MSE): {overall_corr_mse:.4f}\")\n",
        "        print(f\"  • Mean Absolute Error (MAE): {overall_corr_mae:.4f}\")\n",
        "    else:\n",
        "        print(\"  • Correlation-based imputation was not successful for all columns\")\n",
        "    \n",
        "    print(\"\\n📊 REGRESSION-BASED IMPUTATION:\")\n",
        "    print(f\"  • Mean Squared Error (MSE): {overall_reg_mse:.4f}\")\n",
        "    print(f\"  • Mean Absolute Error (MAE): {overall_reg_mae:.4f}\")\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "else:\n",
        "    print(\"\\n❌ No successful imputation runs completed.\")\n",
        "    print(\"Please check if there's sufficient data in the merged dataset.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imputation 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "co2 = pd.read_csv('data/co2.csv')\n",
        "aq = pd.read_csv('data/air_quality.csv')\n",
        "pollution = pd.read_csv('data/pollution.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>avg</th>\n",
              "      <th>jan</th>\n",
              "      <th>feb</th>\n",
              "      <th>mar</th>\n",
              "      <th>apr</th>\n",
              "      <th>may</th>\n",
              "      <th>jun</th>\n",
              "      <th>jul</th>\n",
              "      <th>aug</th>\n",
              "      <th>sep</th>\n",
              "      <th>oct</th>\n",
              "      <th>nov</th>\n",
              "      <th>dec</th>\n",
              "      <th>country_code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>19.000000</td>\n",
              "      <td>20.0</td>\n",
              "      <td>21.5</td>\n",
              "      <td>21.0</td>\n",
              "      <td>18.500000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>16.500000</td>\n",
              "      <td>14.500000</td>\n",
              "      <td>14.500000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>20.5</td>\n",
              "      <td>113.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>AFG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>14.000000</td>\n",
              "      <td>20.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>16.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>ALB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>23.000000</td>\n",
              "      <td>21.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>27.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>26.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>DZA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>18.666667</td>\n",
              "      <td>17.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>19.666667</td>\n",
              "      <td>21.666667</td>\n",
              "      <td>21.333333</td>\n",
              "      <td>23.666667</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>17.666667</td>\n",
              "      <td>16.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>AND</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>14.400000</td>\n",
              "      <td>27.6</td>\n",
              "      <td>31.8</td>\n",
              "      <td>22.3</td>\n",
              "      <td>32.800000</td>\n",
              "      <td>41.600000</td>\n",
              "      <td>17.266667</td>\n",
              "      <td>12.566667</td>\n",
              "      <td>13.166667</td>\n",
              "      <td>13.133333</td>\n",
              "      <td>12.4</td>\n",
              "      <td>15.8</td>\n",
              "      <td>5.4</td>\n",
              "      <td>AGO</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         avg   jan   feb   mar        apr        may        jun        jul  \\\n",
              "0  19.000000  20.0  21.5  21.0  18.500000  16.000000  16.500000  14.500000   \n",
              "1  14.000000  20.0  23.0  13.0  10.000000  11.000000  10.000000  12.000000   \n",
              "2  23.000000  21.0  24.0  20.0  20.000000  15.000000  19.000000  27.000000   \n",
              "3  18.666667  17.0  20.0  19.0  19.666667  21.666667  21.333333  23.666667   \n",
              "4  14.400000  27.6  31.8  22.3  32.800000  41.600000  17.266667  12.566667   \n",
              "\n",
              "         aug        sep   oct    nov    dec country_code  \n",
              "0  14.500000  20.000000  20.5  113.0  114.0          AFG  \n",
              "1  11.000000  12.000000  16.0   11.0   20.0          ALB  \n",
              "2  20.000000  28.000000  26.0   22.0   37.0          DZA  \n",
              "3  21.000000  17.666667  16.0   14.0   14.0          AND  \n",
              "4  13.166667  13.133333  12.4   15.8    5.4          AGO  "
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "aq.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "pollutant\n",
              "PM2.5               116\n",
              "PM10                 71\n",
              "NO2                  55\n",
              "SO2                  55\n",
              "O3                   52\n",
              "CO                   49\n",
              "TEMPERATURE          28\n",
              "PM1                  17\n",
              "RELATIVEHUMIDITY     16\n",
              "UM003                16\n",
              "NO                    5\n",
              "NOX                   5\n",
              "BC                    2\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pollution['pollutant'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Only using top pollutants\n",
        "pollution = pollution[pollution['pollutant'].isin(['PM2.5', 'PM10', 'NO2', 'SO2', 'O3', 'CO'])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "co2.set_index(['country_code'], inplace=True)\n",
        "co2 = co2[~co2.index.get_level_values('country_code').duplicated(keep='last')]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>value</th>\n",
              "      <th>co2_per_capita</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>country_code</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ABW</th>\n",
              "      <td>2016</td>\n",
              "      <td>883.747000</td>\n",
              "      <td>0.008439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AFG</th>\n",
              "      <td>2019</td>\n",
              "      <td>6079.999924</td>\n",
              "      <td>0.000160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AGO</th>\n",
              "      <td>2019</td>\n",
              "      <td>25209.999084</td>\n",
              "      <td>0.000779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ALB</th>\n",
              "      <td>2019</td>\n",
              "      <td>4829.999924</td>\n",
              "      <td>0.001683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AND</th>\n",
              "      <td>2019</td>\n",
              "      <td>500.000000</td>\n",
              "      <td>0.006535</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              year         value  co2_per_capita\n",
              "country_code                                    \n",
              "ABW           2016    883.747000        0.008439\n",
              "AFG           2019   6079.999924        0.000160\n",
              "AGO           2019  25209.999084        0.000779\n",
              "ALB           2019   4829.999924        0.001683\n",
              "AND           2019    500.000000        0.006535"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "co2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [],
      "source": [
        "aq.set_index(['country_code'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "co2['aq'] = aq['avg']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>value</th>\n",
              "      <th>co2_per_capita</th>\n",
              "      <th>aq</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>country_code</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ABW</th>\n",
              "      <td>2016</td>\n",
              "      <td>883.747000</td>\n",
              "      <td>0.008439</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AFG</th>\n",
              "      <td>2019</td>\n",
              "      <td>6079.999924</td>\n",
              "      <td>0.000160</td>\n",
              "      <td>19.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AGO</th>\n",
              "      <td>2019</td>\n",
              "      <td>25209.999084</td>\n",
              "      <td>0.000779</td>\n",
              "      <td>14.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ALB</th>\n",
              "      <td>2019</td>\n",
              "      <td>4829.999924</td>\n",
              "      <td>0.001683</td>\n",
              "      <td>14.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AND</th>\n",
              "      <td>2019</td>\n",
              "      <td>500.000000</td>\n",
              "      <td>0.006535</td>\n",
              "      <td>18.666667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              year         value  co2_per_capita         aq\n",
              "country_code                                               \n",
              "ABW           2016    883.747000        0.008439        NaN\n",
              "AFG           2019   6079.999924        0.000160  19.000000\n",
              "AGO           2019  25209.999084        0.000779  14.400000\n",
              "ALB           2019   4829.999924        0.001683  14.000000\n",
              "AND           2019    500.000000        0.006535  18.666667"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "co2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Suppose your DataFrame has columns: 'country_code', 'year', 'pollutant', 'unit', 'value'\n",
        "\n",
        "pollution_pivot = pollution.pivot_table(\n",
        "    index=['country_code'],   # Rows: one per country per year\n",
        "    columns='pollutant',              # Columns: one per pollutant\n",
        "    values='value'                    # Fill values from the 'value' column\n",
        ").reset_index()\n",
        "\n",
        "# Optional: flatten the column MultiIndex if needed\n",
        "pollution_pivot.columns.name = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>country_code</th>\n",
              "      <th>CO</th>\n",
              "      <th>NO2</th>\n",
              "      <th>O3</th>\n",
              "      <th>PM10</th>\n",
              "      <th>PM2.5</th>\n",
              "      <th>SO2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AFG</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-431.500000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AND</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>4.700000</td>\n",
              "      <td>68.333333</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>7.900000</td>\n",
              "      <td>0.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ARE</td>\n",
              "      <td>306.666667</td>\n",
              "      <td>9.185556</td>\n",
              "      <td>48.202308</td>\n",
              "      <td>247.722222</td>\n",
              "      <td>-204.200000</td>\n",
              "      <td>9.108889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ARG</td>\n",
              "      <td>590.000000</td>\n",
              "      <td>12.500000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>10.400000</td>\n",
              "      <td>6.148884</td>\n",
              "      <td>5.710000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ARM</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10.400000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  country_code          CO        NO2         O3        PM10       PM2.5  \\\n",
              "0          AFG         NaN        NaN        NaN         NaN -431.500000   \n",
              "1          AND  100.000000   4.700000  68.333333   11.000000    7.900000   \n",
              "2          ARE  306.666667   9.185556  48.202308  247.722222 -204.200000   \n",
              "3          ARG  590.000000  12.500000  26.000000   10.400000    6.148884   \n",
              "4          ARM         NaN        NaN        NaN         NaN   10.400000   \n",
              "\n",
              "        SO2  \n",
              "0       NaN  \n",
              "1  0.200000  \n",
              "2  9.108889  \n",
              "3  5.710000  \n",
              "4       NaN  "
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pollution_pivot.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "pollution_pivot.set_index(['country_code'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "for pollutant in pollution_pivot.columns:\n",
        "    co2[pollutant] = pollution_pivot[pollutant]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>value</th>\n",
              "      <th>co2_per_capita</th>\n",
              "      <th>aq</th>\n",
              "      <th>CO</th>\n",
              "      <th>NO2</th>\n",
              "      <th>O3</th>\n",
              "      <th>PM10</th>\n",
              "      <th>PM2.5</th>\n",
              "      <th>SO2</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>country_code</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ABW</th>\n",
              "      <td>2016</td>\n",
              "      <td>883.747000</td>\n",
              "      <td>0.008439</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AFG</th>\n",
              "      <td>2019</td>\n",
              "      <td>6079.999924</td>\n",
              "      <td>0.000160</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-431.5</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AGO</th>\n",
              "      <td>2019</td>\n",
              "      <td>25209.999084</td>\n",
              "      <td>0.000779</td>\n",
              "      <td>14.400000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ALB</th>\n",
              "      <td>2019</td>\n",
              "      <td>4829.999924</td>\n",
              "      <td>0.001683</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AND</th>\n",
              "      <td>2019</td>\n",
              "      <td>500.000000</td>\n",
              "      <td>0.006535</td>\n",
              "      <td>18.666667</td>\n",
              "      <td>100.0</td>\n",
              "      <td>4.7</td>\n",
              "      <td>68.333333</td>\n",
              "      <td>11.0</td>\n",
              "      <td>7.9</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              year         value  co2_per_capita         aq     CO  NO2  \\\n",
              "country_code                                                              \n",
              "ABW           2016    883.747000        0.008439        NaN    NaN  NaN   \n",
              "AFG           2019   6079.999924        0.000160  19.000000    NaN  NaN   \n",
              "AGO           2019  25209.999084        0.000779  14.400000    NaN  NaN   \n",
              "ALB           2019   4829.999924        0.001683  14.000000    NaN  NaN   \n",
              "AND           2019    500.000000        0.006535  18.666667  100.0  4.7   \n",
              "\n",
              "                     O3  PM10  PM2.5  SO2  \n",
              "country_code                               \n",
              "ABW                 NaN   NaN    NaN  NaN  \n",
              "AFG                 NaN   NaN -431.5  NaN  \n",
              "AGO                 NaN   NaN    NaN  NaN  \n",
              "ALB                 NaN   NaN    NaN  NaN  \n",
              "AND           68.333333  11.0    7.9  0.2  "
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "co2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "co2.drop(columns=['year', 'value'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "co2_per_capita    0.000000\n",
              "aq                0.359223\n",
              "CO                0.762136\n",
              "NO2               0.733010\n",
              "O3                0.747573\n",
              "PM10              0.665049\n",
              "PM2.5             0.456311\n",
              "SO2               0.733010\n",
              "dtype: float64"
            ]
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "co2.isna().sum() / len(co2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       LinearRegression  KNNImputer  IterativeImputer\n",
            "aq             1.579789    1.579789          1.579789\n",
            "CO             2.133414    2.133414          2.133414\n",
            "NO2            1.270085    1.270085          1.270085\n",
            "O3             0.531262    0.531262          0.531262\n",
            "PM10           3.684628    3.684628          3.684628\n",
            "PM2.5          0.327614    0.327614          0.327614\n",
            "SO2            6.554661    6.554661          6.554661\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import KNNImputer, IterativeImputer\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "\n",
        "df = co2.copy()\n",
        "\n",
        "def impute_and_evaluate_models(df, mask_fraction=0.4, random_state=42):\n",
        "    np.random.seed(random_state)\n",
        "    results = {}\n",
        "\n",
        "    for col in df.columns:\n",
        "        if df[col].isna().sum() == 0:\n",
        "            continue\n",
        "\n",
        "        known_idx = df[df[col].notna()].index\n",
        "        mask_size = int(len(known_idx) * mask_fraction)\n",
        "        mask_idx = np.random.choice(known_idx, size=mask_size, replace=False)\n",
        "        true_values = df.loc[mask_idx, col].copy()\n",
        "        df.loc[mask_idx, col] = np.nan\n",
        "\n",
        "        predictors = df.columns[df.columns != col]\n",
        "\n",
        "        # 1. Linear Regression\n",
        "        X_train = df.loc[df[col].notna(), predictors].fillna(0)\n",
        "        y_train = df.loc[df[col].notna(), col]\n",
        "        X_pred = df.loc[df[col].isna(), predictors].fillna(0)\n",
        "        lr = LinearRegression()\n",
        "        lr.fit(X_train, y_train)\n",
        "        df.loc[df[col].isna(), col] = lr.predict(X_pred)\n",
        "        mape_lr = mean_absolute_percentage_error(true_values, df.loc[mask_idx, col])\n",
        "\n",
        "        # 2. KNN Imputer\n",
        "        knn_imputer = KNNImputer(n_neighbors=3)\n",
        "        df_knn = df.copy()\n",
        "        df_knn[:] = knn_imputer.fit_transform(df_knn)\n",
        "        mape_knn = mean_absolute_percentage_error(true_values, df_knn.loc[mask_idx, col])\n",
        "\n",
        "        # 3. Iterative Imputer\n",
        "        iter_imputer = IterativeImputer(random_state=random_state)\n",
        "        df_iter = df.copy()\n",
        "        df_iter[:] = iter_imputer.fit_transform(df_iter)\n",
        "        mape_iter = mean_absolute_percentage_error(true_values, df_iter.loc[mask_idx, col])\n",
        "\n",
        "        results[col] = {\n",
        "            \"LinearRegression\": mape_lr,\n",
        "            \"KNNImputer\": mape_knn,\n",
        "            \"IterativeImputer\": mape_iter\n",
        "        }\n",
        "\n",
        "    return results\n",
        "\n",
        "mape_results = impute_and_evaluate_models(df)\n",
        "print(pd.DataFrame(mape_results).T )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
